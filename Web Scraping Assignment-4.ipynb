{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "820fb16a",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71edc953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url=\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'lxml')\n",
    "video_name=[]\n",
    "artist=[]\n",
    "views=[]\n",
    "date=[]\n",
    "rows=soup.find_all('tr')\n",
    "for row in rows:\n",
    "    columns=row.find_all('td')\n",
    "    if len(columns)>4:\n",
    "        video_name.append(columns[1].text.strip())\n",
    "        artist.append(columns[2].text.strip())\n",
    "        views.append(columns[3].text.strip())\n",
    "        date.append(columns[4].text.strip())\n",
    "for i in range(len(video_name)):\n",
    "    print(\"Rank:\",i+1,\"Video:\",video_name[i],\"Artist:\",artist[i],\"Date:\",date[i],\"Views:\",views[i])   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2a8c59",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3afa9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url=\"https://www.bcci.tv/\"\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'lxml')\n",
    "fixture_link=soup.find('a',string=\"Fixtures and Results\")\n",
    "fixture_url=fixture_link['href']\n",
    "response1=requests.get(fixture_url)\n",
    "soup1=BeautifulSoup(response1.text,'lxml')\n",
    "dates=soup1.find_all('div',class_=\"match-dates ng-binding\")\n",
    "seriess=soup1.find_all('h5',class_=\"match-tournament-name ng-binding\")\n",
    "places=soup1.find_all('span ng-if',class_=\"ng-binding ng-scope\")\n",
    "for date in dates:\n",
    "    print(date.text)\n",
    "for series in seriess:\n",
    "    print(series.text)\n",
    "for place in places:\n",
    "    print(place.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38198e5a",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f692e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url=\"http://statisticstimes.com\"\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'lxml')\n",
    "menu_link = soup.find_all('div', class_=\"dropdown\")\n",
    "economy=menu_link[1]\n",
    "\n",
    "# Find all the 'a' tags within this dropdown\n",
    "links = economy.find_all('a')\n",
    "india_url=\"http://statisticstimes.com/\"\n",
    "# Look for the link containing 'India'\n",
    "for link in links:\n",
    "    if 'India' in link.text:\n",
    "        india_url = india_url + link['href']  # Concatenate the URL\n",
    "\n",
    "response1=requests.get(india_url)\n",
    "soup1=BeautifulSoup(response1.text,'lxml')\n",
    "gdp=soup1.find_all('li')\n",
    "for gdp_link in gdp:\n",
    "    states=gdp_link.find('a')\n",
    "    if \"GDP of Indian state\" in states.text:\n",
    "        indianstates_url=states['href']\n",
    "        break\n",
    "final_url=url+\"/economy/\"+indianstates_url\n",
    "\n",
    "response2=requests.get(final_url)\n",
    "soup2=BeautifulSoup(response2.text,'lxml')\n",
    "table=soup2.find('table',id='table_id')\n",
    "ranks=[]\n",
    "state=[]\n",
    "gsdp23_24=[]\n",
    "gsdp22_23=[]\n",
    "share=[]\n",
    "gdp_billion=[]\n",
    "if table:  # Check if table is found\n",
    "    tbody = table.find('tbody')\n",
    "    tr=tbody.find_all('tr')\n",
    "    for row in tr:\n",
    "        td_elements=row.find_all('td')\n",
    "        if td_elements:\n",
    "             print(\"Rank: \",td_elements[0].text, \" Name: \",td_elements[1].text, \" GSDP23-24:\",td_elements[2].text, \" GSDP22-23:\",td_elements[3].text, \"Share: \", td_elements[5].text,\" GDP($billion): \",td_elements[6].text,\"\\n\")\n",
    "        \n",
    "else:\n",
    "    print(\"Table not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96791bda",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffc7bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url=\"http://github.com/\"\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'lxml')\n",
    "menu = soup.find_all('nav',class_=\"HeaderMenu-nav\")\n",
    "for nav in menu:\n",
    "    lis=nav.find_all('li')\n",
    "    for li in lis:\n",
    "        trendings= li.find_all('a')\n",
    "        for trending in trendings:\n",
    "            if trending.text.strip()==\"Trending\":\n",
    "                new_url=trending['href']  \n",
    "response1=requests.get(new_url)\n",
    "soup1=BeautifulSoup(response1.text,'lxml')\n",
    "title=[]\n",
    "desc=[]\n",
    "lang=[]\n",
    "article_box = soup1.find_all('article',class_=\"Box-row\")\n",
    "for box in article_box:\n",
    "    title=box.find('h2').text.strip()\n",
    "    desc=box.find('p').text.strip()\n",
    "    lang_tag=box.find('span',itemprop=\"programmingLanguage\")\n",
    "    lang=lang_tag.text.strip() if lang_tag else print(\"No language specified\")\n",
    "    cont_tag = box.find_all('a', class_='Link--muted')\n",
    "    cont = cont_tag[-1].text.strip() if cont_tag else \"No contributors found\"\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Description: {desc}\")\n",
    "    print(f\"Language: {lang}\\n\")\n",
    "    print(f\"Contributors: {cont}\\n”)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214845cf",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad67e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Initialize the WebDriver (Assuming Chrome in this case)\n",
    "driver = webdriver.Chrome(executable_path='/path/to/chromedriver')\n",
    "\n",
    "# Go to Billboard website\n",
    "url = \"https://www.billboard.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(3)\n",
    "\n",
    "# Click on the \"Charts\" button (ensure it's visible)\n",
    "charts_button = driver.find_element(\"xpath\", \"//a[@aria-label='Charts']\")\n",
    "charts_button.click()\n",
    "\n",
    "# Wait for the dropdown to load and click on the \"Hot 100\" link\n",
    "time.sleep(2)\n",
    "hot_100_button = driver.find_element(\"xpath\", \"//a[contains(text(), 'Hot 100')]\")\n",
    "hot_100_button.click()\n",
    "\n",
    "# Wait for the Hot 100 page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Parse the Hot 100 page using BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "# Find all the song entries on the page\n",
    "songs = soup.find_all('li', class_='o-chart-results-list__item')\n",
    "\n",
    "# Initialize empty lists to store data\n",
    "song_names = []\n",
    "artist_names = []\n",
    "last_week_ranks = []\n",
    "peak_ranks = []\n",
    "weeks_on_chart = []\n",
    "\n",
    "# Loop through each song and extract details\n",
    "for song in songs:\n",
    "    # Song name\n",
    "    song_name = song.find('h3', class_='c-title').text.strip()\n",
    "    song_names.append(song_name)\n",
    "\n",
    "    # Artist name\n",
    "    artist_name = song.find('span', class_='c-label').text.strip()\n",
    "    artist_names.append(artist_name)\n",
    "\n",
    "    # Last week rank\n",
    "    last_week = song.find('span', class_='c-label', text='Last Week').find_next('span').text\n",
    "    last_week_ranks.append(last_week)\n",
    "\n",
    "    # Peak rank\n",
    "    peak_rank = song.find('span', class_='c-label', text='Peak Rank').find_next('span').text\n",
    "    peak_ranks.append(peak_rank)\n",
    "\n",
    "    # Weeks on chart\n",
    "    weeks = song.find('span', class_='c-label', text='Weeks on Chart').find_next('span').text\n",
    "    weeks_on_chart.append(weeks)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Display the results\n",
    "for i in range(len(song_names)):\n",
    "    print(f\"{i+1}. Song: {song_names[i]}, Artist: {artist_names[i]}, Last Week Rank: {last_week_ranks[i]}, Peak Rank: {peak_ranks[i]}, Weeks on Chart: {weeks_on_chart[i]}”)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a503237a",
   "metadata": {},
   "source": [
    "# Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908a66ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Target URL\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "\n",
    "# Request the webpage\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "# Find the table containing the book details\n",
    "table = soup.find('table')\n",
    "\n",
    "# Lists to store the scraped data\n",
    "book_names = []\n",
    "authors = []\n",
    "volumes_sold = []\n",
    "publishers = []\n",
    "genres = []\n",
    "\n",
    "# Loop through the table rows (skipping the header row)\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    cells = row.find_all('td')\n",
    "    \n",
    "    # Extracting details from each column\n",
    "    book_names.append(cells[0].text.strip())    # Book Name\n",
    "    authors.append(cells[1].text.strip())       # Author\n",
    "    volumes_sold.append(cells[2].text.strip())  # Volumes Sold\n",
    "    publishers.append(cells[3].text.strip())    # Publisher\n",
    "    genres.append(cells[4].text.strip())        # Genre\n",
    "\n",
    "# Display the results\n",
    "for i in range(len(book_names)):\n",
    "    print(f\"Book: {book_names[i]}, Author: {authors[i]}, Volumes Sold: {volumes_sold[i]}, Publisher: {publishers[i]}, Genre: {genres[i]}”)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ea00a8",
   "metadata": {},
   "source": [
    "# Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b93de42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# IMDb URL\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "# Finding all the list items containing TV series details\n",
    "series_list = soup.find_all('div', class_='lister-item-content')\n",
    "\n",
    "# Lists to store details\n",
    "names = []\n",
    "year_spans = []\n",
    "genres = []\n",
    "run_times = []\n",
    "ratings = []\n",
    "votes = []\n",
    "\n",
    "# Loop through each series in the list\n",
    "for series in series_list:\n",
    "    # Extract Name\n",
    "    name = series.h3.a.text.strip()\n",
    "    names.append(name)\n",
    "    \n",
    "    # Extract Year span\n",
    "    year_span = series.h3.find('span', class_='lister-item-year').text.strip('()')\n",
    "    year_spans.append(year_span)\n",
    "    \n",
    "    # Extract Genre\n",
    "    genre = series.find('span', class_='genre').text.strip()\n",
    "    genres.append(genre)\n",
    "    \n",
    "    # Extract Run time\n",
    "    run_time = series.find('span', class_='runtime').text.strip()\n",
    "    run_times.append(run_time)\n",
    "    \n",
    "    # Extract Rating\n",
    "    rating = series.find('div', class_='inline-block ratings-imdb-rating').strong.text.strip()\n",
    "    ratings.append(rating)\n",
    "    \n",
    "    # Extract Votes\n",
    "    vote = series.find('span', attrs={'name': 'nv'})['data-value']\n",
    "    votes.append(vote)\n",
    "\n",
    "# Print extracted details\n",
    "for i in range(len(names)):\n",
    "    print(f\"Name: {names[i]}, Year span: {year_spans[i]}, Genre: {genres[i]}, Run time: {run_times[i]}, Ratings: {ratings[i]}, Votes: {votes[i]}”)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9602bd9a",
   "metadata": {},
   "source": [
    "# Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8c4bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Base URL of UCI Machine Learning Repository\n",
    "url = \"https://archive.ics.uci.edu/ml/index.php\"\n",
    "\n",
    "# Send request and get response from URL\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "# Find the \"Show All Dataset\" link\n",
    "all_datasets_link = soup.find('a', string=\"View All Datasets\")['href']\n",
    "\n",
    "# Concatenate the base URL with the relative link\n",
    "datasets_url = \"https://archive.ics.uci.edu/ml/\" + all_datasets_link\n",
    "\n",
    "# Send request and get response from the All Datasets page\n",
    "response2 = requests.get(datasets_url)\n",
    "soup2 = BeautifulSoup(response2.text, 'lxml')\n",
    "\n",
    "# Find all dataset entries in the page\n",
    "datasets = soup2.find_all('tr')[1:]  # Skip the header row\n",
    "\n",
    "# Iterate over the datasets to get the required information\n",
    "for dataset in datasets:\n",
    "    columns = dataset.find_all('td')\n",
    "    \n",
    "    # Extract details for each dataset\n",
    "    dataset_name = columns[0].a.text.strip()\n",
    "    data_type = columns[1].text.strip()\n",
    "    task = columns[2].text.strip()\n",
    "    attribute_type = columns[3].text.strip()\n",
    "    no_of_instances = columns[4].text.strip()\n",
    "    no_of_attributes = columns[5].text.strip()\n",
    "    year = columns[6].text.strip()\n",
    "    \n",
    "    # Print the extracted information\n",
    "    print(f\"Dataset Name: {dataset_name}\")\n",
    "    print(f\"Data Type: {data_type}\")\n",
    "    print(f\"Task: {task}\")\n",
    "    print(f\"Attribute Type: {attribute_type}\")\n",
    "    print(f\"No of Instances: {no_of_instances}\")\n",
    "    print(f\"No of Attributes: {no_of_attributes}\")\n",
    "    print(f\"Year: {year}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
